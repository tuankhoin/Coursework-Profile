{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import glob\n",
    "from Yinsh.yinsh_model import YinshGameRule \n",
    "from Yinsh.yinsh_utils import *\n",
    "import numpy as np\n",
    "import re\n",
    "import copy\n",
    "import random\n",
    "from keras.callbacks import TensorBoard\n",
    "import time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_reward(old_state,new_state, agent_index):\n",
    "    # Check if any ring is removed or this is the end of game\n",
    "    if new_state.rings_won[agent_index] == 3:\n",
    "        return 10\n",
    "    # If the opponent won, return -10\n",
    "    elif new_state.rings_won[int(not agent_index)] == 3:\n",
    "        return -10\n",
    "    # Check if any ring is removed\n",
    "    if new_state.rings_won[0] != old_state.rings_won[0] or new_state.rings_won[1] != old_state.rings_won[1]:\n",
    "        own_change = new_state.rings_won[agent_index] - old_state.rings_won[agent_index]\n",
    "        opp_change = new_state.rings_won[int(not agent_index)] - old_state.rings_won[int(not agent_index)]\n",
    "        return 3*own_change - 3*opp_change\n",
    "    \n",
    "    return 0\n",
    "    # Finally, return the potential function\n",
    "    # return potential_func(new_state, agent_index) - potential_func(old_state, agent_index)\n",
    "        \n",
    "\n",
    "\"\"\"\n",
    "game_rule_static = YinshGameRule(2)\n",
    "def potential_func(state, agent_index):\n",
    "    # Calculate potential function for a given state using heuristic by Khoi\n",
    "    if state.rings_to_place > 0: return 0\n",
    "    return 0.5*(ChainHeuristic(state,agent_index,0) - 0.1*ChainHeuristic(state,agent_index,1))\n",
    "\n",
    "def ChainHeuristic( s, agent_index, opponent_perspective: int = 0, pts: dict = {0:0, 1:0, 2:1, 3:3, 4:5, 5:6}):\n",
    "    '''\n",
    "    Heuristic that returns score based on the feasible, unblocked sequences of markers.\n",
    "\n",
    "    params\n",
    "    ---\n",
    "    - s: Game state\n",
    "    - opponent_perspective: Whose view are we looking at? Our view (0) or opponent's view (1)?\n",
    "    - pts: A dictionary mapping chains to corresponding point\n",
    "    '''\n",
    "    # What color are we looking at?\n",
    "    view = agent_index ^ opponent_perspective\n",
    "    tot_mark = 0\n",
    "    ring = str(RING_1 if view else RING_0)\n",
    "    counter = str(CNTR_1 if view else CNTR_0)\n",
    "    opponent_counter = str(CNTR_0 if view else CNTR_1)\n",
    "    # Get all markers first\n",
    "    # Return the list of all locations of a specific type of grid on the board\n",
    "    lookup_pos = lambda n,b : list(zip(*np.where(b==n)))\n",
    "    markers = lookup_pos(CNTR_1 if view else CNTR_0,s.board)\n",
    "    lines_of_interest = set()\n",
    "    # Get all lines with markers\n",
    "    for m in markers:\n",
    "        for line in ['v','h','d']:\n",
    "            lines_of_interest.add(tuple(game_rule_static.positionsOnLine(m,line)))\n",
    "    # For each line that has markers, see if a feasible chain exists\n",
    "    # R: ring       M: my marker     M_opponent: opponent marker\n",
    "    for p in lines_of_interest:\n",
    "        p_str  = ('').join([str(s.board[i]) for i in p])\n",
    "        # Chains of 5 mixed R/M\n",
    "        for st in re.findall(f'[{ring}{counter}]*',p_str):\n",
    "            # How many rings needed to move to get 5-marker streak?\n",
    "            if len(st)>4: tot_mark+=pts[5-st.count(ring)]\n",
    "        # Incomplete but feasible EMPTY-R-nM (not summing up to 5 yet)\n",
    "        for st in re.findall(f'{str(EMPTY)*3}{ring}{counter*1}'\n",
    "                            + f'|{str(EMPTY)*2}{ring}{counter*2}'\n",
    "                            + f'|{str(EMPTY)*1}{ring}{counter*3}',p_str):\n",
    "            tot_mark+=pts[st.count(counter)]\n",
    "        # Incomplete but feasible nM-R-EMPTY (not summing up to 5 yet)\n",
    "        for st in re.findall(f'{counter*1}{ring}{str(EMPTY)*3}'\n",
    "                            + f'|{counter*2}{ring}{str(EMPTY)*2}'\n",
    "                            + f'|{counter*3}{ring}{str(EMPTY)*1}',p_str):\n",
    "            tot_mark+=pts[st.count(counter)]\n",
    "        # R-nM_opponent-EMPTY\n",
    "        for st in re.findall(f'{ring}{opponent_counter*1}{str(EMPTY)*3}'\n",
    "                            + f'|{ring}{opponent_counter*2}{str(EMPTY)*2}'\n",
    "                            + f'|{ring}{opponent_counter*3}{str(EMPTY)*1}'\n",
    "                            # Needs at least a space to land after jumping over\n",
    "                            + f'|{ring}{opponent_counter*4}{str(EMPTY)*1}',p_str):\n",
    "            tot_mark+=pts[st.count(counter)]\n",
    "        # EMPTY-nM_opponent-R\n",
    "        for st in re.findall(f'{str(EMPTY)*3}{opponent_counter*1}{ring}'\n",
    "                            + f'|{str(EMPTY)*2}{opponent_counter*2}{ring}'\n",
    "                            + f'|{str(EMPTY)*1}{opponent_counter*3}{ring}'\n",
    "                            # Needs at least a space to land after jumping over\n",
    "                            + f'|{str(EMPTY)*1}{opponent_counter*4}{ring}',p_str):\n",
    "            tot_mark+=pts[st.count(counter)]\n",
    "    return tot_mark\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "replay_memory = [] # Observed state, action, reward, next state\n",
    "folder_path = 'replays'\n",
    "\n",
    "for filename in glob.glob(os.path.join(folder_path, '*.replay')):\n",
    "    replay = pickle.load(open(filename,'rb'),encoding=\"bytes\")\n",
    "    # Initial game state\n",
    "    num_of_agent = replay[\"num_of_agent\"]\n",
    "    game_rule = YinshGameRule(num_of_agent)\n",
    "    # Run the replay\n",
    "    # Old state and action for agents\n",
    "    old_state = [None,None]\n",
    "    old_action = [None,None]\n",
    "    end_unexpected = False\n",
    "    for item in replay[\"actions\"]:\n",
    "        (index, info), = item.items()\n",
    "        selected = info[\"action\"]\n",
    "        agent_index = info[\"agent_id\"]\n",
    "        if old_state[agent_index] is not None:\n",
    "            current_state = copy.deepcopy(game_rule.current_game_state)\n",
    "            reward = calculate_reward(old_state[agent_index],current_state, agent_index)\n",
    "            replay_memory.append((old_state[agent_index], old_action[agent_index], reward,current_state,agent_index,False))\n",
    "        old_state[agent_index] = copy.deepcopy(game_rule.current_game_state)\n",
    "        old_action[agent_index] = selected\n",
    "        # print(info)\n",
    "        game_rule.current_agent_index = agent_index          \n",
    "        action_candidates = game_rule.getLegalActions(game_rule.current_game_state, agent_index)\n",
    "\n",
    "        if selected not in action_candidates:\n",
    "            print(\"Error: illegal action\")\n",
    "            old_state[agent_index] = None\n",
    "            old_action[agent_index] = None\n",
    "            end_unexpected = True\n",
    "            break\n",
    "        game_rule.update(selected)\n",
    "    # Game ends, do a final update\n",
    "    if not game_rule.gameEnds():\n",
    "        print(filename)\n",
    "    assert game_rule.gameEnds() or end_unexpected\n",
    "    if not end_unexpected:\n",
    "        for agent_index in range(num_of_agent):\n",
    "            current_state = copy.deepcopy(game_rule.current_game_state)\n",
    "            reward = calculate_reward(old_state[agent_index],current_state, agent_index)\n",
    "            replay_memory.append((old_state[agent_index], old_action[agent_index], reward,current_state,agent_index,True))  \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57068"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(replay_memory) # 57068"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-24 13:15:23.479421: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(keras.Input(shape=(11, 11, 4)))\n",
    "model.add(keras.layers.Conv2D(filters=64,kernel_size = 3,activation = 'relu'))\n",
    "model.add(keras.layers.Conv2D(filters=128,kernel_size = 3,activation = 'relu'))\n",
    "model.add(keras.layers.Conv2D(filters=256,kernel_size = 3,activation = 'relu'))\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Dense(units=1933,activation = 'softmax'))\n",
    "model.compile(loss=tf.keras.losses.Huber(), optimizer=tf.keras.optimizers.RMSprop(learning_rate=0.0001), metrics=['accuracy'])\n",
    "\n",
    "model.load_weights('main_model_dqn_replay.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_state_for_net(state,agent_index):\n",
    "    # Reshape the state to be [11, 11, 4] => 4 dimensions are our ring, our marker, opponent's ring, opponent's marker\n",
    "    reshaped_state = np.zeros((11, 11, 4),dtype=np.int8)\n",
    "    assert state.board.shape[0] == state.board.shape[1] == 11\n",
    "    for i in range (0, 11):\n",
    "        for j in range (0, 11):\n",
    "            # If the pos is illegal, update all layers to be 0\n",
    "            if state.board[i][j] == 5:\n",
    "                for z in range (0, 4):\n",
    "                    reshaped_state[i][j][z] = 0\n",
    "            elif state.board[i][j] == 1: # RING_0\n",
    "                # If the pos is our ring, update layer 0 to be 1\n",
    "                if agent_index == 0:\n",
    "                    reshaped_state[i][j][0] = 1\n",
    "                # If the pos is opponent's ring, update layer 2 to be 1\n",
    "                else:\n",
    "                    reshaped_state[i][j][2] = 1\n",
    "            elif state.board[i][j] == 2: # CNTR_0\n",
    "                # If the pos is our marker, update layer 1 to be 1\n",
    "                if agent_index == 0:\n",
    "                    reshaped_state[i][j][1] = 1\n",
    "                # If the pos is opponent's marker, update layer 3 to be 1\n",
    "                else:\n",
    "                    reshaped_state[i][j][3] = 1\n",
    "            elif state.board[i][j] == 3: # RING_1\n",
    "                # If the pos is our ring, update layer 0 to be 1\n",
    "                if agent_index == 1:\n",
    "                    reshaped_state[i][j][0] = 1\n",
    "                # If the pos is opponent's ring, update layer 2 to be 1\n",
    "                else:\n",
    "                    reshaped_state[i][j][2] = 1\n",
    "            elif state.board[i][j] == 4: # CNTR_1\n",
    "                # If the pos is our marker, update layer 1 to be 1\n",
    "                if agent_index == 1:\n",
    "                    reshaped_state[i][j][1] = 1\n",
    "                # If the pos is opponent's marker, update layer 3 to be 1\n",
    "                else:\n",
    "                    reshaped_state[i][j][3] = 1\n",
    "    return reshaped_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "board = np.zeros((11,11), dtype=np.int8)\n",
    "for pos in ILLEGAL_POS:\n",
    "    board[pos] = ILLEGAL\n",
    "# A dictionary that map an action into an index in NN output\n",
    "action_to_index_dict = {}\n",
    "count = 0\n",
    "legal_pos = []\n",
    "for i in range(11):\n",
    "    for j in range(11):\n",
    "        if board[i,j] != ILLEGAL:\n",
    "            legal_pos.append((i,j))\n",
    "            action_to_index_dict[(i,j)] = count\n",
    "            count += 1\n",
    "assert len(legal_pos) == 85\n",
    "for pos in legal_pos:\n",
    "    for line in ['v', 'h', 'd']:\n",
    "        if line == 'h':\n",
    "            for i in range(11):\n",
    "                if board[pos[0], i] != ILLEGAL and i != pos[1]:\n",
    "                    # From pos[0],pos[1] to pos[0],i\n",
    "                    action_to_index_dict[(pos[0],pos[1],pos[0],i)] = count\n",
    "                    count += 1\n",
    "        elif line == 'v':\n",
    "            for i in range(11):\n",
    "                if board[i, pos[1]] != ILLEGAL and i != pos[0]:\n",
    "                    # From pos[0],pos[1] to pos[0],i\n",
    "                    action_to_index_dict[(pos[0],pos[1],i,pos[1])] = count\n",
    "                    count += 1\n",
    "        elif line == 'd':\n",
    "            for i in range(-10, 11):\n",
    "                if i == 0: continue\n",
    "                if (0 <= pos[0]+i <= 10 and 0 <= pos[1]-i <= 10 and board[pos[0]+i, pos[1]-i] != ILLEGAL):\n",
    "                    action_to_index_dict[(pos[0],pos[1],pos[0]+i,pos[1]-i)] = count\n",
    "                    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the learning process\n",
    "class ModifiedTensorBoard(TensorBoard):\n",
    "\n",
    "    # Overriding init to set initial step and writer (we want one log file for all .fit() calls)\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.step = 1\n",
    "        self.writer = tf.summary.create_file_writer(self.log_dir)\n",
    "        self._log_write_dir = self.log_dir\n",
    "    # Overriding this method to stop creating default log writer\n",
    "    def set_model(self, model):\n",
    "        self.model = model\n",
    "        self._train_dir = os.path.join(self._log_write_dir, 'train')\n",
    "        self._train_step = self.model._train_counter\n",
    "        self._val_dir = os.path.join(self._log_write_dir, 'validation')\n",
    "        self._val_step = self.model._test_counter\n",
    "        self._should_write_train_graph = False\n",
    "\n",
    "    # Overrided, saves logs with our step number\n",
    "    # (otherwise every .fit() will start writing from 0th step)\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        self.update_stats(**logs)\n",
    "\n",
    "    # Overrided\n",
    "    # We train for one batch only, no need to save anything at epoch end\n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        pass\n",
    "\n",
    "    # Overrided, so won't close writer\n",
    "    def on_train_end(self, _):\n",
    "        pass\n",
    "\n",
    "    # Creates writer, writes custom metrics and closes writer\n",
    "    def update_stats(self, **stats):\n",
    "        self._write_logs(stats, self.step)\n",
    "\n",
    "    def _write_logs(self, logs, index):\n",
    "        with self.writer.as_default():\n",
    "            for name, value in logs.items():\n",
    "                tf.summary.scalar(name, value, step=index)\n",
    "                self.step += 1\n",
    "                self.writer.flush()\n",
    "\n",
    "tensorboard = ModifiedTensorBoard(log_dir=\"replay_train_logs/{}\".format(int(time.time())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15000/15000 [17:57:24<00:00,  4.31s/it]  \n"
     ]
    }
   ],
   "source": [
    "# Get a minibatch of random samples from memory replay table\n",
    "MINIBATCH_SIZE = 50\n",
    "for i in tqdm(range(15000)):\n",
    "    minibatch = random.sample(replay_memory, MINIBATCH_SIZE)\n",
    "   #  old_state(not flatted) , old_action, reward,new state(not flatted), agent index,game end\n",
    "    X = []\n",
    "    y = []\n",
    "    for experience in minibatch:\n",
    "        # Get stored values\n",
    "        agent_index = experience[4]\n",
    "        old_state = reshape_state_for_net(experience[0],agent_index)\n",
    "        old_state_prediction = model.predict(np.expand_dims(old_state,axis=0))[0]\n",
    "        action_index = None\n",
    "        action = experience[1]\n",
    "        if action[\"type\"] == \"place ring\":\n",
    "            action_index = action_to_index_dict[action[\"place pos\"]]\n",
    "        elif action[\"type\"] == \"place and move\" or  action[\"type\"] == \"place, move, remove\":\n",
    "            action_index = action_to_index_dict[(action[\"place pos\"][0],action[\"place pos\"][1],action[\"move pos\"][0],action[\"move pos\"][1])]\n",
    "\n",
    "        reward = experience[2]\n",
    "        new_state = np.expand_dims(reshape_state_for_net(experience[3],agent_index), axis=0) \n",
    "        # Get prediction of the new state\n",
    "        new_state_prediction = model.predict(new_state)[0]\n",
    "        # Get the target value\n",
    "        target_value = None\n",
    "        if experience[5]:\n",
    "            target_value = reward\n",
    "        else:\n",
    "            target_value = reward + 0.9 * np.max(new_state_prediction)\n",
    "        # Get the target value for the old state\n",
    "        target_f = old_state_prediction\n",
    "        target_f[action_index] = target_value\n",
    "        # Train the network\n",
    "        X.append(old_state)\n",
    "        y.append(target_f)\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    model.fit(X, y, epochs=1, verbose=0, callbacks=[tensorboard] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('main_model_dqn_replay.h5')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "45871442c49025fdd20fa9948e5254bdb2344e7fbe4be0810f4af0f237a16cc7"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('COMP90042NLP')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
